_target_: src.modules.models.valor.valor_architectures.VideoMAEv2VideoLadderAdapterVideoFeatureMapConvPoolerVideoMambaDecoderSharedTextMultimodalBertEncoderDecoder

data_info:
  
  dataset_name: ${datamodule.dataset_info.dataset_name}
  class_names: ${datamodule.dataset_info.class_names} 
  num_classes: ${datamodule.dataset_info.num_classes}
  ignore_index: ${datamodule.dataset_info.ignore_index}
  background_index: ${datamodule.dataset_info.background_index}


video_encoder:

  video_backbone:       
            
    pretrained: True
    pretrained_checkpoints_dir: ${paths.pretrained_checkpoints_dir}
    pretrained_checkpoint_name: "vit-base_pretrain-videomaev2-k710-valor-ufc101_fps-24_frames-16_swstride-6.pth"
    unfreeze_backbone: False # False, nth layer, True  
    instantiate:      
      img_size: 224
      num_frames: 16
      patch_size: 16
      depth: 12
      embed_dims: 768
      mlp_ratio: 4
      norm_cfg:       
        eps: 1.0e-06
        type: "LN"        
      num_heads: 12
      qkv_bias: True
      use_mean_pooling: True # Not really used but 
      return_feat_map: False
      return_feat_map_foreach_layer: False #! Back to true for normal mode

  video_ladder_adapter:

    depth: null
    conv_adapter:    
      embed_dims: null
      scaling_factor: 0.5
      kernel_size: 3
      dilation: 1


  video_feature_map_conv_pooler:
    embed_dims: null
    
video_decoder:

  adapter:      
    num_layers: 1
    encoder_embed_dims: null
    decoder_embed_dims: 768
  
  base:
    num_layers: 8
    d_state_expansion_factor: 16
    d_conv: 4
    dt_rank: 'auto'
    block_expansion_factor: 2

  head:  
    num_layers: 1
    num_classes: 22

shared_text_multimodal_encoder_decoder:

  bert:        
    checkpoint_path: "${paths.pretrained_checkpoints_dir}/bert-base-uncased.bin"
    attention_probs_dropout_prob: 0.1
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    hidden_size: 768
    initializer_range: 0.02
    intermediate_size: 3072
    max_position_embeddings: 512
    num_attention_heads: 12
    num_hidden_layers: 12
    type_vocab_size: 2
    vocab_size: 30522

  bert_tokenizer:
    vocab_path: "${paths.pretrained_checkpoints_dir}/bert-base-uncased-vocab.txt"

  token_masker:
    mask_token: null
    mask_prob: 0.6
    range_start: 106
    range_end: 30522

contrastive_heads:

  contra_head_text:
    embed_dims: null
    contra_dims: 512  

  contra_head_video:  
    embed_dims: null
    contra_dims: 512

weights_init: null

criterions:
  
  losses:
  
    oad_loss:
      output: oad
      weight: 1.0
      instantiate:          
        _target_: src.modules.criterions.criterions.OADLoss
        num_classes: ${module.model.data_info.num_classes} 
        use_weights: False
        dataset_name: ${module.model.data_info.dataset_name} 
        reduction: mean

    # multimodal_captioning_loss:
    #   output: multimodal_captioning
    #   weight: 1.0
    #   instantiate:          
    #     _target_: src.modules.criterions.criterions.MultimodalCaptioningLoss

    # contrastive_multimodal_alignment_loss:
    #   output: contrastive_multimodal_alignment
    #   weight: 1.0
    #   instantiate:
    #     _target_: src.modules.criterions.criterions.ContrastiveMultimodalAlignmentLoss
    #     starting_temp: 0.07


metrics:

  train:

    mAP:
      instantiate:
        _target_: src.modules.metrics.metrics.MeanAveragePrecision
        num_classes: ${module.model.data_info.num_classes}
        class_names: ${module.model.data_info.class_names}
        background_index: ${module.model.data_info.background_index} #TODO think about if we want to ignore also class backgrounds (maybe switch to extended_background_indices)
        ignore_index: ${module.model.data_info.ignore_index}
        merge_ignore_index_into_background_index: False
        return_dict: False

    Acc:
      instantiate: 
        _target_:  src.modules.metrics.metrics.MulticlassAccuracyWrapper
        num_classes: ${module.model.data_info.num_classes}
        ignore_index: ${module.model.data_info.ignore_index}
        top_k: 1
        average: "micro" # micro: normal accuracy , macro: accuracy for each class and then mean, none: accuracy for each class 
  
  valid:
    mAP:
      instantiate: 
        _target_: src.modules.metrics.metrics.MeanAveragePrecision
        num_classes: ${module.model.data_info.num_classes}
        class_names: ${module.model.data_info.class_names}
        background_index: ${module.model.data_info.background_index} #TODO think about if we want to ignore also class backgrounds (maybe switch to extended_background_indices)
        ignore_index: ${module.model.data_info.ignore_index}
        merge_ignore_index_into_background_index: False
        return_dict: False


    Acc:
      instantiate: 
        _target_:  src.modules.metrics.metrics.MulticlassAccuracyWrapper
        num_classes: ${module.model.data_info.num_classes}
        ignore_index: ${module.model.data_info.ignore_index}
        top_k: 1
        average: "micro" # micro: normal accuracy , macro: accuracy for each class and then mean, none: accuracy for each class 

optimizer:
  _target_: torch.optim.AdamW
  lr: 1.0e-4
  weight_decay: 1.0e-5
  betas: [0.9, 0.999]

scheduler:
  _target_: src.modules.schedulers.schedulers.LinearWarmupCosineAnnealingLR
  warmup_epochs: 5
  warmup_start_lr: 1.0e-6
  eta_min: 0.0
  