# @package _global_

defaults:
- override /callbacks: lrm_mc #learning_rate_monitor #lrm_mc
- override /datamodule: thumos_train_ufvfcd_val_ufvfd_notransforms
- override /module: multigpu_singleoptim_module
- override /module/model: videomambadecoder_sharedtextmultimodalbertencoderdecoder_adamw_lwcdscheduler 

checkpoint_path: ../experiments/OAD/train_thumos_tufvfcd_vufvfd_valor/checkpoints/3_train_thumos_tufvfcd_vufvfd_valor/41-8285.ckpt

trainer:
  _target_: pytorch_lightning.Trainer
  default_root_dir: ${paths.output_dir}
  min_epochs: 1 # prevents early stopping
  max_epochs: 100
  
  strategy: ddp_find_unused_parameters_true #ddp #ddp_find_unused_parameters_true
  accelerator: gpu
  devices: [7]
  num_nodes: 1
  sync_batchnorm: True
  precision: 32
  # reload training dataloaders every epoch
  reload_dataloaders_every_n_epochs: 1 
  # perform a validation loop every N training epochs
  check_val_every_n_epoch: 1
  # log at every nth step
  log_every_n_steps: 1
  # set True to ensure deterministic results
  # makes training slower but gives more reproducibility than just setting seeds
  deterministic: True
  enable_checkpointing: True # set to true and add checkpointing callback to checkpoint best validation model.
  detect_anomaly: False
  use_distributed_sampler: False
  
  # JUST FOR TESTING DATALOADING TIME
  # max_steps: 10
  # profiler: "simple"



env:
  seed: 0
  max_threads: 24

logger:
  _target_: pytorch_lightning.loggers.wandb.WandbLogger
  entity: "schweizertomas" # set to name of your wandb team
  offline: False
  anonymous: null # enable anonymous logging
  project: OAD
  group: test_thumos_tufvfcd_vufvfd_valor
  job_type: train
  save_dir: ${paths.output_dir}
  log_model: True
  name: null
  notes: null 